{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built-in Modules\n",
    "\n",
    "Most of the functions in Python are from modules, instead of builtin (global) functions. That's how they keep language syntax simple and stay flexible to meet different needs. \n",
    "\n",
    "There are built-in modules in all Python versions and distributions. However, chances are some distribution (especially on embedded environments) may remove some modules to save space. In that case, you might have to install some missing modules manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global configs & imports\n",
    "import gzip, csv, json, os, os.path, glob, multiprocessing, time, pickle, re, random\n",
    "from pprint import pprint\n",
    "\n",
    "PATH = {\n",
    "    \"data\": \"../data\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "\n",
    "Human can only read text-based files, and that's why text-based formats such as yaml, json, XML, emerged in recent years. They became common even as data exchange formats, which is a territory that binary formats dominated for compactness and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## csv\n",
    "\n",
    "CSV or Comma-Separated Values, is probably the most ancient format that can express data tables (structured). However, the lack of detailed standard (RFC4180 is quite rough) makes it less suitable for non-ASCII characters. \n",
    "\n",
    "`csv.DictReader` wraps functions of a CSV Reader. It assumes first line to be csv header, exposed as `.fieldnames` attribute, and returns the lines one bye one as `dict` when iterated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 lines loaded\n"
     ]
    }
   ],
   "source": [
    "filename = os.path.join(PATH['data'], 'small.csv')\n",
    "with open(filename) as fp:\n",
    "    reader = csv.DictReader(fp)\n",
    "    lines = list(reader)\n",
    "\n",
    "print(\"%d lines loaded\" % len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, loading entire file into memory at once (what above code does) may not be efficient, especially when dealing with large datasets. In this case, iterating over these lines can be a better idea. \n",
    "\n",
    "BTW, both `file` object and `csv.DictReader` supports iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 lines loaded\n"
     ]
    }
   ],
   "source": [
    "with open(filename) as fp:\n",
    "    reader = csv.DictReader(fp)\n",
    "    line_count = 0\n",
    "    \n",
    "    for item in reader:\n",
    "        # process that item\n",
    "        line_count += 1\n",
    "\n",
    "print(\"%d lines loaded\" % line_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON\n",
    "\n",
    "As JavaScript became popular this decade, so do JSON (JavaScript Object Notation) as data exchange format on the web. It's more human readable and compact compared to XML, and more expressive to YAML.\n",
    "\n",
    "JSON syntax is stricter than JavaScript:\n",
    "\n",
    "* Double quotes only\n",
    "* Quotes required for Object keys\n",
    "* No trailing comma allowed\n",
    "* No comment\n",
    "\n",
    "Sample syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "students_json = \"\"\"[\n",
    "    {\n",
    "        \"name\": \"Joe\",\n",
    "        \"age\": 3,\n",
    "        \"siblings\": [\n",
    "            \"Tom\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Tom\",\n",
    "        \"age\": 5,\n",
    "        \"siblings\": [\n",
    "            \"Joe\"\n",
    "        ]\n",
    "    }\n",
    "]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that data in JSON syntax happends to be valid in Python! But in most cases, we treat them as strings, and convert to and from native data structures, just like other exchange formats. To do so, we have to `import json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'age': 3, 'name': 'Joe', 'siblings': ['Tom']},\n",
      " {'age': 5, 'name': 'Tom', 'siblings': ['Joe']}]\n"
     ]
    }
   ],
   "source": [
    "pprint(json.loads(students_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Q1\": [\"January\", \"February\", \"March\"], \"Q2\": [\"April\", \"May\", \"June\"], \"Q4\": [\"October\", \"November\", \"December\"], \"Q3\": [\"July\", \"August\", \"September\"]}\n"
     ]
    }
   ],
   "source": [
    "d = {\n",
    "    \"Q1\": [\"January\", \"February\", \"March\"],\n",
    "    \"Q2\": [\"April\", \"May\", \"June\"],\n",
    "    \"Q3\": [\"July\", \"August\", \"September\"],\n",
    "    \"Q4\": [\"October\", \"November\", \"December\"],\n",
    "}\n",
    "\n",
    "print(json.dumps(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass `indent` to json.dumps for human-friendly outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Q1\": [\n",
      "        \"January\",\n",
      "        \"February\",\n",
      "        \"March\"\n",
      "    ],\n",
      "    \"Q2\": [\n",
      "        \"April\",\n",
      "        \"May\",\n",
      "        \"June\"\n",
      "    ],\n",
      "    \"Q4\": [\n",
      "        \"October\",\n",
      "        \"November\",\n",
      "        \"December\"\n",
      "    ],\n",
      "    \"Q3\": [\n",
      "        \"July\",\n",
      "        \"August\",\n",
      "        \"September\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(d, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting them back to native data structures should yield the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d == json.loads(json.dumps(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "Since JSON supports two collection types (`Object` and `Array`, equivalent to Python `dict` and `list`), passing unsupported types (`set`, `object`, ...) may produce unexpected results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pickle\n",
    "\n",
    "Unlike `csv` or `json`, `pickle` is unique to Python, and was used for serialization and unserialization instead of human-readable data transport. As a consequence, `pickle` returns byte strings, is able to handle most Python objects, and is guaranteed for backward compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x80\\x03}q\\x00(X\\x02\\x00\\x00\\x00Q1q\\x01]q\\x02(X\\x07\\x00\\x00\\x00Januaryq\\x03X\\x08\\x00\\x00\\x00Februaryq\\x04X\\x05\\x00\\x00\\x00Marchq\\x05eX\\x02\\x00\\x00\\x00Q2q\\x06]q\\x07(X\\x05\\x00\\x00\\x00Aprilq\\x08X\\x03\\x00\\x00\\x00Mayq\\tX\\x04\\x00\\x00\\x00Juneq\\neX\\x02\\x00\\x00\\x00Q4q\\x0b]q\\x0c(X\\x07\\x00\\x00\\x00Octoberq\\rX\\x08\\x00\\x00\\x00Novemberq\\x0eX\\x08\\x00\\x00\\x00Decemberq\\x0feX\\x02\\x00\\x00\\x00Q3q\\x10]q\\x11(X\\x04\\x00\\x00\\x00Julyq\\x12X\\x06\\x00\\x00\\x00Augustq\\x13X\\t\\x00\\x00\\x00Septemberq\\x14eu.'\n"
     ]
    }
   ],
   "source": [
    "print(pickle.dumps(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q1': ['January', 'February', 'March'], 'Q2': ['April', 'May', 'June'], 'Q4': ['October', 'November', 'December'], 'Q3': ['July', 'August', 'September']}\n"
     ]
    }
   ],
   "source": [
    "print (pickle.loads(pickle.dumps(d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(pickle.loads(pickle.dumps(d)) == d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pickel` provides similar API compared to `json`:\n",
    "\n",
    "* `load()` to load from file object\n",
    "* `loads()` from byte string\n",
    "* `dump()` to write to file object\n",
    "* `dumps()` to a byte string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex (re)\n",
    "\n",
    "Regular Expression is like a chain saw, powerful for a certain scale as long as you can handle it. RegEx itself is so complex that there are books on it. With limited time here, we'll only talk about basics of `re`, and how you should be using it.\n",
    "\n",
    "There are several functions exposed from module `re`, but normally we just call `re.compile()` to turn patterns into compiled regular expressions. That because the most common usecase is to create a few regex patterns and search for them from lots of strings. This pattern can save significant computation in practice.\n",
    "\n",
    "Matching regex itself is also much slower than simple string comparisons. Don't use regex on these cases, such as `a == b`, `a in b`, or a.startswith(b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern Syntax\n",
    "\n",
    "In some languages, backslash (\\) is used to escape symbols in strings. Backslashes themselves have to be escaped (\\\\) to represent one backslash. Regular expression also uses backslash to escape symbols, so we need to type four backslashes to represent one (plain backslash) in regex pattern in those languages.\n",
    "\n",
    "Unfortunately, Python is too smart and it tries to escape backslashes automatically if the following symbol doesn't have special meaning in strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\\\d' '\\\\d'\n",
      "'\\t' '\\\\t'\n"
     ]
    }
   ],
   "source": [
    "print(repr(\"\\d\"), repr(\"\\\\d\"))\n",
    "print(repr(\"\\t\"), repr(\"\\\\t\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's why we prepend 'r' in front of pattern strings to tell Python those are **raw** strings, and they will be interpreted as-is. Then our regex would be easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\\\d' '\\\\\\\\d'\n",
      "'\\\\t' '\\\\\\\\t'\n"
     ]
    }
   ],
   "source": [
    "print(repr(r\"\\d\"), repr(r\"\\\\d\"))\n",
    "print(repr(r\"\\t\"), repr(r\"\\\\t\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like Python, there are special symbols in regex that represents new meanings if occuring escaped (after backslash) or unescaped. Some of them are:\n",
    "\n",
    "* **(** ... **)** parentheses: \n",
    "    * Define logical units\n",
    "    * Signify regex engine to pass this segment to output (retrieved via `groups()` call on `Match` object)\n",
    "* **(?:**...**)**\n",
    "    * Non-capturing group, works like ..., but it's not captured as ouptut\n",
    "* **[** ... **]** square brackets:\n",
    "    * Occurrence of any of character within\n",
    "    * Different escape rules for characters within\n",
    "    * Dash means a range of characters. Eg., a-z, A-Z, 0-9\n",
    "* **.** dot: Matches to any character\n",
    "* **\\d**: Any number\n",
    "* **\\w**: Any alphabet (Ambiguous behavior for unicode characters)\n",
    "* **\\s**: Spaces (including tab, carriage return, line feed, spaces, ...)\n",
    "* Occurrences\n",
    "    * **?**: The character or logical unit before **?** must appear 0 or 1 time.\n",
    "    * **\\***: The character or logical unit before **\\*** must appear arbitrary times (including 0).\n",
    "    * **+**: The character or logical unit before **+** must appear 1 or more times.\n",
    "    * **{** N, M **}**: The character or logical unit before **{** must appear between N and M times (inclusive).\n",
    "    * **{** N **}**: The character or logical unit before **{** must appear exactly N times.\n",
    "    * **{** N, **}**: The character or logical unit before **{** can appear N or more times.\n",
    "* Position\n",
    "    * **^**: Match to start of string\n",
    "    * **$**: Match to end of string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Pattern` API\n",
    "\n",
    "After `compile()`, we then match strings against our `Pattern` object with these functions:\n",
    "\n",
    "* `.findall()`: Searches for all non-overlapping matches, and return them as `list` of `strings`. Greedy matches apply.\n",
    "* `.finditer()`: Like `findall()`, but returns a generator.\n",
    "* `.search()`: Search for first occurrence after pos (defualt=0), and return it as Match object.\n",
    "* `.match()`: Like `search()`, but it matches from beginning, as if there's an \"^\" at the beginning in pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192.168.1.1\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "ip = \"192.168.1.1\"\n",
    "ptrn_ip = r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\"\n",
    "\n",
    "matcher = re.compile(ptrn_ip)\n",
    "result = matcher.search(ip)\n",
    "\n",
    "print(result.group())\n",
    "print(result.groups())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192.168.1.1\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ptrn_ip_strict = r\"^%s$\" % ptrn_ip\n",
    "ip_sentence = \"My ip addr is %s.\" % ip\n",
    "print(re.search(ptrn_ip, ip_sentence).group())\n",
    "print(re.search(ptrn_ip_strict, ip_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`re.search` returns Match object when pattern is found in string, and `None` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(re.search(r\"\\d+\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of the directive you use in the pattern, regex groups are always strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192.168.1.1\n",
      "('192', '168', '1', '1')\n"
     ]
    }
   ],
   "source": [
    "result = re.search(r\"(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\", ip)\n",
    "print(result.group())\n",
    "print(result.groups())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ptrn = re.compile(r\"((?:m|i|r|g|t|c|cc|cg|hi|hs)\\d)\" \\\n",
    "                  \"\\.(micro|small|medium|(?:\\d*x)?large)\")\n",
    "\n",
    "def t(s):\n",
    "    ret = ptrn.search(s)\n",
    "    print(ret.groups() if ret else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('m2', 'small')\n",
      "('t2', 'xlarge')\n",
      "('c4', 'large')\n",
      "('t8', '64xlarge')\n"
     ]
    }
   ],
   "source": [
    "# Positive test cases\n",
    "\n",
    "t(r\"Launched m2.small in us-east-1\")\n",
    "t(r\"t2.xlarge are twice as powerful than t2.large\")\n",
    "t(r\"c4.large\")\n",
    "t(r\"Rumor has it that t8.64xlarge is coming next Q3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Negative test cases\n",
    "\n",
    "t(r\"k2.small\")\n",
    "t(r\"g2.XLARGE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filesystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builtin `file` object\n",
    "\n",
    "Builtin `file` objects can be created with `file` function calls. There is a cursor pointing to current offset, and all operations (read, write, iterate, ...) will work based on this cursor.\n",
    "\n",
    "There are two functions to manipulate with the cursor:\n",
    "\n",
    "* `tell()`: Return offset in bytes\n",
    "* `seek()`: There are 3 modes for seek (POSIX), aka Whence:\n",
    "    * SEEK_SET: from start of that file\n",
    "    * SEEK_CUR: relative to current offset\n",
    "    * SEEK_END: from end of file\n",
    "\n",
    "`whence` is defined as constants in most implementations. Check the manual before using them. In Python, `file.seek(offset, whence=0)` recognizes 0 as SEEK_SET, 1 SEEK_CUR, and 2 a SEEK_END. To avoid confusion, offset must be zero when whence in (1,2). Call `fseek(0, 2)` will move your cursor to the end of the file, which is useful when appending to a file opened with mode `\"r+\"`.\n",
    "\n",
    "As a common practice, functions that take `file` objects as inputs should call `file.tell()` to save cursor offset, and restore via `file.seek(offset)` before retuning to avoid side-effects, unless that's the function's primary effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## os.path\n",
    "\n",
    "`os.path` is a standalone module that you have to `import` explicitly. It wraps most filesystem functions, such as:\n",
    "\n",
    "* `dirname()`: Return dirname to a path\n",
    "* `abspath()`: Returns absolute path to a relative or fragmented one.\n",
    "* `realpath()`: like abspath, but it resolves symbolic links.\n",
    "* `join()`: Joins fragments into a str representing legit path. Takes care of platform-specific delimeters.\n",
    "* `getatime()`, `getctime()`, `getmtime()`\n",
    "* `getsize()`: Gets file size, relies on low level call wrapped as `os.stat`.\n",
    "* `isfile()`, `isdir()`, `islink()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## os.walk\n",
    "\n",
    "Searches for directories and files recursively from a root directory. Return value from `os.walk` is a generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object walk at 0x0000000004D950D8>\n"
     ]
    }
   ],
   "source": [
    "scanpath = os.path.join(\".\")\n",
    "tmp = os.walk(scanpath)\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.',\n",
      "  ['.ipynb_checkpoints'],\n",
      "  ['00.Common.ipynb',\n",
      "   '01.Python Basics.ipynb',\n",
      "   '01q.ipynb',\n",
      "   '02.Builtin Modules.ipynb',\n",
      "   '03.More on Python.ipynb',\n",
      "   '04.3rd-Party Modules.ipynb']),\n",
      " ('.\\\\.ipynb_checkpoints',\n",
      "  [],\n",
      "  ['00.Common-checkpoint.ipynb',\n",
      "   '01.Python Basics-checkpoint.ipynb',\n",
      "   '01q-checkpoint.ipynb',\n",
      "   '02.Builtin Modules-checkpoint.ipynb',\n",
      "   '04.3rd-Party Modules-checkpoint.ipynb'])]\n"
     ]
    }
   ],
   "source": [
    "pprint(list(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## glob\n",
    "\n",
    "Module `glob` handles filename globbing, and is handy listing files under a directory. There are two functions in `glob`:\n",
    "\n",
    "* `glob.glob`: Returns filenames as a list\n",
    "* `glob.iglob`: Returns a generator that iterates over filenames\n",
    "\n",
    "When dealing with folders containing a lot of files, `iglob` can be more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.\\\\00.Common.ipynb', '.\\\\01.Python Basics.ipynb', '.\\\\01q.ipynb', '.\\\\02.Builtin Modules.ipynb', '.\\\\03.More on Python.ipynb', '.\\\\04.3rd-Party Modules.ipynb']\n"
     ]
    }
   ],
   "source": [
    "print(glob.glob(\"%s/*\" % scanpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.\\\\00.Common.ipynb', '.\\\\01.Python Basics.ipynb', '.\\\\01q.ipynb', '.\\\\02.Builtin Modules.ipynb', '.\\\\03.More on Python.ipynb', '.\\\\04.3rd-Party Modules.ipynb']\n"
     ]
    }
   ],
   "source": [
    "print(list(glob.iglob(\"%s/*\" % scanpath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./..\\\\Common\\\\git.ipynb', './..\\\\PH 201508\\\\00.Common.ipynb', './..\\\\PH 201508\\\\01.Python Basics.ipynb', './..\\\\PH 201508\\\\01q.ipynb', './..\\\\PH 201508\\\\02.Builtin Modules.ipynb', './..\\\\PH 201508\\\\03.More on Python.ipynb', './..\\\\PH 201508\\\\04.3rd-Party Modules.ipynb']\n"
     ]
    }
   ],
   "source": [
    "print(glob.glob(\"%s/../*/*.ipynb\" % scanpath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# os\n",
    "\n",
    "Wraps lots of POSIX functions. For example, `nice()`, `open()` (Low-level one that returns file descriptor numbers instead of `file` object), `getpid()`, `execl()` (use 3rd-party library `subprocess` instead), `chdir()`, `setuid()`, `umask()`, `urandom()`.\n",
    "\n",
    "Most of them are low-level functions, and you should consider high-level wrappers instead (unless you know what your're doing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random\n",
    "\n",
    "Hosts functions about random, including:\n",
    "\n",
    "* `randint(a, b)`: Returns a random number in range [a, b] (inclusive on both ends). Supports long (bigint).\n",
    "* `random()`: Returns a float in interval [0, 1)\n",
    "* `choice(a)`: Returns an element from list a\n",
    "* `randrange(start, stop, step)`: Fixes randint() that both ends are inclusive. \n",
    "* `sample(l, n)`: Pick n distinct elements from l. Duplicates are allowed in l, and they were treated as different entries.\n",
    "* `shuffle(l)`: Shuffles entries in l:list in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roll a dice and you get 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Roll a dice and you get %d\" % random.randint(1,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Requests: urllib, urllib2, urllib3\n",
    "\n",
    "These are intermediate libraries for http[s] with more new features. Old ones were still kept for compatibility reasons. urllib2 takes `Request` objects in its action API's, while urllib3 supports connetion pool.\n",
    "\n",
    "Most of the time, you should be dealing with high-level HTTP wrppers, like Requests (we'll talk about a few days later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gzip\n",
    "\n",
    "The API for gzip is very straightforward because there's no folder structure in gzip. As a consequence, gzip files can be manipulated with `gzip.open()`, and it behaves almost like builtin `file` object.\n",
    "\n",
    "There can be some compatibility issues between Python2/3 with gzip. By default, gzip opens files in binary mode, and data read from those files is `str` in Python2 and `byte` in Python3. Supplying `mode` as text (t) when calling `gzip.open()` can solve the compatibility issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050001 lines loaded\n"
     ]
    }
   ],
   "source": [
    "with gzip.open(\"../data/large-one-column.csv.gz\", 'rt') as fp:\n",
    "    print(\"%d lines loaded\" % len(list(fp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zipfile\n",
    "\n",
    "Unlike `gzip`, `zip` handles file structure internally and requires another tier of abstraction to handle it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing\n",
    "\n",
    "Due to physical limits, it's harder to raise clock frequency beyond 4GHz. On the contrary, Moore's Law continues. And now we have 8+ cores on mobile devices, and soon there can be 64+ cores on desktops. The initial approach in most languages was threads, but sharing memory addresses can cause a lot of trouble. Introducing GIL (Global Intepreter Lock) in Python does solve this issue, but it also makes threading less favorable for only one thread in a process can enter run state at a given time. \n",
    "\n",
    "That's why `multiprocessing` came to the rescue in Python 2.6. It mimics the API from `muiltithreading` to make migration easier. Processes can be initiated from `multiprocessing` and then distributed across CPU cores. It also includes helper classes to handle IPC (Inter-process communication) and data synchronization transparently behind the scene. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Multiprocessing doesn't work well in IPython notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process\n",
    "\n",
    "Python will launch a new `Process` and attach entry point (a function) to that thread. Since the function will be imported from that thread, it's always a good practice to protect modular statements with `if __name__==\"__main\": blah...` block. The main process can then call `Process.start()` to start the child process, and `Process.join()` to wait until child process ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start_mp.py\n",
    "\n",
    "from multiprocessing import Process\n",
    "\n",
    "def f(name):\n",
    "    print ('hello %s' % name)\n",
    "\n",
    "if __name__ == '__main__' and False:\n",
    "    p = Process(target=f, args=('bob',))\n",
    "    p.start()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pool\n",
    "\n",
    "`multiprocessing.Pool` offers a simple interface that mimics built-in function `map()`. With `Pool.map()`, we can easily parallelize functions as long as it ...\n",
    "1. takes one argument \n",
    "2. can be imported from modular space \n",
    "3. without side effect when imported\n",
    "4. arguments can be pickle'd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = multiprocessing.Pool(2)\n",
    "inputs = list(range(30000))\n",
    "\n",
    "def square_it(x):\n",
    "    return x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convential, single thread approach\n",
    "results = map(p, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multiprocess, Twice as fast\n",
    "if False:\n",
    "    results = p.map(square_it, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access functions that require more than one argument, we can define another wrapper that takes one parameter that unpacks the parameter and call the desired function, and pack the variables before passing them to `Pool.map()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPC Helpers\n",
    "\n",
    "There are some classes in `multiprocessing` that helps us to handle IPC (inter-process communication). They handle locks behind the scene to prevent racing conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queue\n",
    "\n",
    "Exposing similar API to `Queue.Queue`, `multiprocessing.Queue` handles data exchange between processes. Usually we create `Queue` from main process, and pass the instance to two groups of processes, namely producers and consumers. Producers `Queue.put()` messages to the `Queue`, while consumers `Queue.get()` from it.\n",
    "\n",
    "`Queue.put()` and `Queue.get()` operations can be blocking or non-blocking. If `Queue.get()` timeouts, it will raise `Queue.Empty` (Exception defined in module `Queue`, not `multiprocessing`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipe\n",
    "\n",
    "Unlike `Queue`, `Pipe`s are bi-directional. That makes it more like a low-level communication protocol, and the developer should define his/her own processing logic on top of `Pipe`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gotcha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error tracking\n",
    "\n",
    "Programs using multiprocessing are harder to trace, for Exceptions helpful to debug were raised from the child processes. The main process that Python were attached to will only know the failure, not the raised Exceptions.\n",
    "\n",
    "Reverting from `Pool.map()` to builtin `map()` can help, for their API are identical. And those with `Process`, the best approach would be writing logs with `logging` that we'll cover later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dangling processes\n",
    "\n",
    "There are chances that child processes may hang, leaving the entire program unresponsive. The most common causes are ...\n",
    "\n",
    "1. Deadlocks in your application logic\n",
    "2. Hanging requests without proper timeouts\n",
    "3. Infinite loop in child process (eg., inappropriate retries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging\n",
    "\n",
    "Like log4j for Java, `logging` is the standard logging facility for logging in Python. You can create and configure the default (top-level) logger for your process, and sub-loggers as well for fine-grained control. Each logger can have its own Filters and Handlers, and the Handlers then format the log message and output desired destination.\n",
    "\n",
    "By default, logging outputs via `StreamingHandler` to stdout, but the output pipe is configurable.\n",
    "\n",
    "Common **output handlers** are:\n",
    "\n",
    "* `StreamingHandler`: Output to streams such as stdout, stderr, or str (via StringIO). It can support file streams, but we have dedicated Handlers for files.\n",
    "* `FileHandler`: Output to file.\n",
    "* `WatchedFileHandler`: Reopens destination if filename is changed elsewhere. Useful working with logrotate. Don't use it on Windows.\n",
    "* `RotatingFileHandler`, `TimedRotatingFileHandler`: Rotates and removes old logs, in case you don't want logrotate.\n",
    "* `SysLogHandler`, `NTEventLogHandler`: Self-explanatory.\n",
    "\n",
    "Builtin levels are:\n",
    "\n",
    "* **CRITICAL** 50\n",
    "* **ERROR** 40\n",
    "* **WARNING** 30\n",
    "* **INFO** 20\n",
    "* **DEBUG** 10\n",
    "* **NOTSET** 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def test_logger(logger):\n",
    "    logger.debug(\"Some debug message\")\n",
    "    logger.info(\"Some information\")\n",
    "    logger.warn(\"Some warning message\")\n",
    "    logger.error(\"Houston, we have a problem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Demo-02:Some warning message\n",
      "ERROR:Demo-02:Houston, we have a problem\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig()\n",
    "test_logger(logging.getLogger(\"Demo-02\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:Demo-02:Some debug message\n",
      "INFO:Demo-02:Some information\n",
      "WARNING:Demo-02:Some warning message\n",
      "ERROR:Demo-02:Houston, we have a problem\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(\"Demo-02\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "test_logger(logger)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
